### 1. Introduction
提出观点：好的模型不仅仅是表现得好，还要可解释。

解释性指的是我们可以了解模型在干什么，怎样得到最终结果的。模型的解释性可以有助于优化模型。

但是目前在很多领域表现优异的模型大多为深度神经网络，例如其在图像和语音识别方面表现能力显著。这些模型复杂、难以解释。

#### 为什么使用影响函数？
原本要观察训练数据的影响，需要做的是将训练数据删除或者细微修改，然后重新训练。但是这个过程计算代价非常昂贵。所以提出稳健统计学中的影响函数，将样本的变化直接映射到模型的变化。

#### 影响函数自身存在的问题及解决方法
影响函数Influence function需要昂贵的二阶导数计算，并假设模型的可微性和凸性，这限制了它们在现代环境中的适用性，因为实际应用中模型通常是不可微分的，非凸的和高维的。 

论文证明了可以使用二阶优化技术（Pearlmutter，1994; Martens，2010; Agarwal等，2016）来有效地估计影响函数，并且即使在可微性和凸性的基本假设不成立的情况下，它们仍然是准确可用的。（比较准确的近似）

#### Introduction的整体结构
下面将详细介绍每段的主要内容：
1. 提出机器学习中关键的问题:‘模型是如何得出特定的预测结果？’ ，即阐述可解释性的重要性
2. 提出目前存在的问题是表现好的模型如深度学习网络太过复杂、解释性低的问题。提问：“模型到底从何而来？”
3. 提出本文针对“模型从何而来”的问题，回溯到训练数据，分析训练数据的影响
4. 如何衡量训练数据的影响？为避免昂贵的重训练代价，引入影响函数
5. 影响函数存在的问题及解决方法
6. 影响函数的应用

---
### Approach

#### 影响函数是什么？
影响函数定义为模型参数的导数：
![Influence function](https://github.com/lidanxu/store-positioning/blob/master/images/Influence_functions.png)

其中海森矩阵 ![Hessian](https://github.com/lidanxu/store-positioning/blob/master/images/Hessian.png)

具体公式含义见论文

#### influence function vs Euclidean distance
